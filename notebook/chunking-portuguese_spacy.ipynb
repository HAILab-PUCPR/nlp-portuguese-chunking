{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando spacy e fazendo download do corpus para português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U spacy\n",
    "!python -m spacy download pt_core_news_md\n",
    "import spacy\n",
    "import pt_core_news_md\n",
    "import re\n",
    "\n",
    "nlp = pt_core_news_md.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceWhiteSpaces(str):\n",
    "    return re.sub('\\s{2,}',' ',str)\n",
    "\n",
    "def tokenizaFrase(frase, lower):\n",
    "    frase = frase.replace('  ',' . ').replace(':',' ').replace('+',' . ').replace('#',' ').replace('(',' (').replace(')',') ').replace('[',' [').replace(']','] ')\n",
    "    frase = replaceWhiteSpaces(frase.strip())\n",
    "    if lower==1:\n",
    "        frase=frase.lower()\n",
    "    doc = nlp(frase)\n",
    "    return doc\n",
    "    \n",
    "def isTokenFimChunk(pos):\n",
    "    if pos != 'VERB' and pos != 'AUX' and pos != 'PUNCT' and pos != 'PRON' and pos != 'SPACE':    \n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def isTokenImportant(pos):\n",
    "    if pos == 'NOUN' or pos == 'PROPN' or pos == 'ADJ':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def deveInserirToken(tamanhoTokens, pos):\n",
    "    if tamanhoTokens!=0 or (tamanhoTokens==0 and (pos != 'ADP' and pos != 'DET' and pos != 'SCONJ' and pos != 'CCONJ')):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def deveManterToken(pos):\n",
    "    if pos != 'DET' and pos != 'SCONJ' and pos != 'CCONJ' and pos != 'ADP':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_np(frase, lower):\n",
    "        doc = tokenizaFrase(frase, lower)\n",
    "        lista_chunks = []\n",
    "        all_novo_chunk = []\n",
    "        temNoun=0\n",
    "        tokens_chunk = []\n",
    "        for num, token in enumerate(doc):\n",
    "            if not isTokenFimChunk(token.pos_):\n",
    "                if deveInserirToken(len(tokens_chunk), token.pos_):\n",
    "                    tokens_chunk.append(token)\n",
    "                if isTokenImportant(token.pos_):\n",
    "                    temNoun=1\n",
    "            else:\n",
    "                if len(tokens_chunk)!=0 and temNoun == 1:\n",
    "                    novo_chunk = ''\n",
    "                    deve_manter = 0\n",
    "                    # retirando ultimos tokens do chunk se não forem importantes\n",
    "                    for i in range(len(tokens_chunk)-1,-1,-1):\n",
    "                        pos = tokens_chunk[i].pos_\n",
    "                        texto = tokens_chunk[i].text\n",
    "                        if deveManterToken(pos):\n",
    "                            deve_manter = 1\n",
    "                        if deve_manter==1:\n",
    "                            novo_chunk = texto+' '+novo_chunk\n",
    "                    all_novo_chunk.append(novo_chunk.strip())\n",
    "                    lista_chunks.append(tokens_chunk)\n",
    "                    tokens_chunk = []\n",
    "                    temNoun = 0\n",
    "                else:\n",
    "                        tokens_chunk = []\n",
    "                        temNoun = 0\n",
    "        return all_novo_chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Frase original:---\n",
      "\n",
      "Data de Criação do Documento: 22/04/2014   Dispneia importante aos esforços + dor tipo peso no peito no esforço. Obeso, has, icc  c # cintilografia miocardica para avaliar angina.\n",
      "\n",
      "\n",
      "---Chunks da frase:---\n",
      "\n",
      "['Data de Criação do Documento 22/04/2014', 'Dispneia importante aos esforços', 'dor tipo peso no peito no esforço', 'Obeso', 'has', 'icc', 'cintilografia miocardica', 'angina']\n"
     ]
    }
   ],
   "source": [
    "#ex = 'Há um único quadro de leitura aberto iniciado por códon de metionina de 1.458 nt no quadro com uma homeobox e uma repetição CAX, e o quadro de leitura aberto está previsto para codificar uma proteína de 51.659 daltons.'\n",
    "ex = 'Data de Criação do Documento: 22/04/2014   Dispneia importante aos esforços + dor tipo peso no peito no esforço. Obeso, has, icc  c # cintilografia miocardica para avaliar angina.'\n",
    "#ex = 'Em uso de insulina nph, metformina, sinvastatina 40mg 2 cp, carvedilol 6,25mg, losartana 50mg 2 x dia, AAS 100mg, clopidogrel 75mg. Tabagista há mais de 40 anos. Hoje fuma 8 cigarros por dia. Antes 2 carteiras/dia. Nega queixas.  O # LAboratorial 07/08/2013: HB: 12,9; Hematocrito 37,5; glicemia: 144; microalbuminuria: 2030,7 mg.  Cintilografia de perfusao miocardica agosto/13: Sem evidencias de isquemia miocardica, fç do VE preservada com hipocinesia septal.'\n",
    "#ex='AVC EM JUGULAR ESQUERDA, SEM SINAIS FLOGÍSTICOS, COM SOROTERAPIA EM CURSO. RELATA ELIMINAÇÕES VESICAIS E INTESTINAIS. RIM TRANSPLANTADO.'\n",
    "\n",
    "print('---Frase original:---\\n')\n",
    "print(ex)\n",
    "print('\\n\\n---Chunks da frase:---\\n')\n",
    "print(get_np(ex, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
