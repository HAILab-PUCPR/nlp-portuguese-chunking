{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscando o modelo de Postagger no repositório do Hugging Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "# modelo BioBERTpt treinado com o MacMorpho\n",
    "# http://nilc.icmc.usp.br/macmorpho/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d06165ef4d4cae930e753ad09d5417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/709M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c0fe134fe14eee8351204f9ea448bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ced353f9a74f04b9ae3b58782f8e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0770f4b51e497eba86d93ac8e62f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"pucpr-br/postagger-bio-portuguese\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('pucpr-br/postagger-bio-portuguese')\n",
    "\n",
    "nlp_token_class = pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self, text, pos):\n",
    "        self.text = text\n",
    "        self.pos = pos\n",
    "    def get_text(self):\n",
    "        return self.text\n",
    "    def get_pos(self):\n",
    "        return self.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceWhiteSpaces(str):\n",
    "    return re.sub('\\s{2,}',' ',str)\n",
    "\n",
    "def tokenizaFrase(frase, lower):\n",
    "    frase = frase.replace('  ',' . ').replace(':',' ').replace('+',' . ').replace('#',' ').replace('(',' (').replace(')',') ').replace('[',' [').replace(']','] ')\n",
    "    frase = replaceWhiteSpaces(frase.strip())\n",
    "    if lower==1:\n",
    "        frase=frase.lower()\n",
    "    #doc = nlp(frase)\n",
    "    doc = nlp_token_class(frase)\n",
    "    lista_obj=[]\n",
    "    for d in doc:\n",
    "        pos = d['entity_group']\n",
    "        pos = getTipoTag(pos)\n",
    "        text = frase[d['start']:d['end']]\n",
    "        obj = Token(text, pos)\n",
    "        lista_obj.append(obj)\n",
    "    return lista_obj\n",
    "    \n",
    "def getTipoTag(tag):\n",
    "    # ADJ é igual\n",
    "    if tag == 'V':\n",
    "        return 'VERB'\n",
    "    if tag == 'N':\n",
    "        return 'NOUN'\n",
    "    if tag == 'PU':\n",
    "        return 'PUNCT'\n",
    "    if tag == 'PROPESS':\n",
    "        return 'PRON'\n",
    "    if tag == 'NPROP':\n",
    "        return 'NOUN'   \n",
    "    if tag == 'PCP':\n",
    "        return 'AUX'  \n",
    "    if tag == 'VAUX':\n",
    "        return 'VERB'\n",
    "    if tag == 'ART':\n",
    "        return 'DET'\n",
    "    if tag == 'KS':\n",
    "        return 'SCONJ'\n",
    "    if tag == 'PREP' or tag == 'PREP+ART':\n",
    "        return 'ADP'\n",
    "    if tag == 'KC':\n",
    "        return 'CCONJ'\n",
    "    if tag == 'ADJ':\n",
    "        return 'ADV'\n",
    "    \n",
    "    \n",
    "    \n",
    "def isTokenFimChunk(pos):\n",
    "    if pos != 'VERB' and pos != 'AUX' and pos != 'PUNCT' and pos != 'PRON':    \n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def isTokenImportant(pos):\n",
    "    if pos == 'NOUN' or pos == 'PROPN' or pos == 'ADJ':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def deveInserirToken(tamanhoTokens, pos):\n",
    "    if tamanhoTokens!=0 or (tamanhoTokens==0 and (pos != 'ADP' and pos != 'DET' and pos != 'SCONJ' and pos != 'CCONJ')):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def deveManterToken(pos):\n",
    "    if pos != 'DET' and pos != 'SCONJ' and pos != 'CCONJ' and pos != 'ADP':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_np(frase, lower):\n",
    "        doc = tokenizaFrase(frase, lower)\n",
    "        lista_chunks = []\n",
    "        all_novo_chunk = []\n",
    "        temNoun=0\n",
    "        tokens_chunk = []\n",
    "        for num, token in enumerate(doc):\n",
    "            if not isTokenFimChunk(token.get_pos()):\n",
    "                if deveInserirToken(len(tokens_chunk), token.get_pos()):\n",
    "                    tokens_chunk.append(token)\n",
    "                if isTokenImportant(token.get_pos()):\n",
    "                    temNoun=1\n",
    "            else:\n",
    "                if len(tokens_chunk)!=0 and temNoun == 1:\n",
    "                    novo_chunk = ''\n",
    "                    deve_manter = 0\n",
    "                    # retirando ultimos tokens do chunk se não forem importantes\n",
    "                    for i in range(len(tokens_chunk)-1,-1,-1):\n",
    "                        pos = tokens_chunk[i].get_pos()\n",
    "                        texto = tokens_chunk[i].get_text()\n",
    "                        if deveManterToken(pos):\n",
    "                            deve_manter = 1\n",
    "                        if deve_manter==1:\n",
    "                            novo_chunk = texto+' '+novo_chunk\n",
    "                    all_novo_chunk.append(novo_chunk.strip())\n",
    "                    lista_chunks.append(tokens_chunk)\n",
    "                    tokens_chunk = []\n",
    "                    temNoun = 0\n",
    "                else:\n",
    "                        tokens_chunk = []\n",
    "                        temNoun = 0\n",
    "        return all_novo_chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Frase original:---\n",
      "\n",
      "Data de Criação do Documento: 22/04/2014   Dispneia importante aos esforços + dor tipo peso no peito no esforço. Obeso, has, icc  c # cintilografia miocardica para avaliar angina.\n",
      "\n",
      "\n",
      "---Chunks da frase:---\n",
      "\n",
      "['Data de Criação do Documento 22', '04', '2014', 'Dispneia importante aos esforços', 'dor tipo peso no peito no esforço', 'Obeso', 'has', 'icc', 'cintilografia miocardica', 'angina']\n"
     ]
    }
   ],
   "source": [
    "#ex = 'Há um único quadro de leitura aberto iniciado por códon de metionina de 1.458 nt no quadro com uma homeobox e uma repetição CAX, e o quadro de leitura aberto está previsto para codificar uma proteína de 51.659 daltons.'\n",
    "ex = 'Data de Criação do Documento: 22/04/2014   Dispneia importante aos esforços + dor tipo peso no peito no esforço. Obeso, has, icc  c # cintilografia miocardica para avaliar angina.'\n",
    "#ex = 'Em uso de insulina nph, metformina, sinvastatina 40mg 2 cp, carvedilol 6,25mg, losartana 50mg 2 x dia, AAS 100mg, clopidogrel 75mg. Tabagista há mais de 40 anos. Hoje fuma 8 cigarros por dia. Antes 2 carteiras/dia. Nega queixas.  O # LAboratorial 07/08/2013: HB: 12,9; Hematocrito 37,5; glicemia: 144; microalbuminuria: 2030,7 mg.  Cintilografia de perfusao miocardica agosto/13: Sem evidencias de isquemia miocardica, fç do VE preservada com hipocinesia septal.'\n",
    "#ex='AVC EM JUGULAR ESQUERDA, SEM SINAIS FLOGÍSTICOS, COM SOROTERAPIA EM CURSO. RELATA ELIMINAÇÕES VESICAIS E INTESTINAIS. RIM TRANSPLANTADO.'\n",
    "\n",
    "print('---Frase original:---\\n')\n",
    "print(ex)\n",
    "print('\\n\\n---Chunks da frase:---\\n')\n",
    "print(get_np(ex, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ADJ: adjective\n",
    "ADP: adposition\n",
    "ADV: adverb\n",
    "AUX: auxiliary\n",
    "CCONJ: coordinating conjunction\n",
    "DET: determiner\n",
    "INTJ: interjection\n",
    "NOUN: noun\n",
    "NUM: numeral\n",
    "PART: particle\n",
    "PRON: pronoun\n",
    "PROPN: proper noun\n",
    "PUNCT: punctuation\n",
    "SCONJ: subordinating conjunction\n",
    "SYM: symbol\n",
    "VERB: verb\n",
    "X: other\n",
    "\n",
    "https://universaldependencies.org/u/pos/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
